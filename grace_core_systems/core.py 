############################
# core.py - Cognitive Core #
############################

import json
import time
import joblib
import torch
import numpy as np
import semver
import psutil
import networkx as nx
from enum import Enum, auto
from pydantic import BaseModel, Field, validator
from typing import Dict, Any, Optional, List, Union
from pathlib import Path
from datetime import datetime
from flask import Flask, jsonify, request
import threading
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("GraceCore")

# --- Canonical Model Semantics ---
class ModelCategory(Enum):
    SEMANTIC = auto()
    SYMBOLIC = auto()
    CONNECTIONIST = auto()
    HYBRID = auto()

class ModelComplexity(Enum):
    LITE = auto()     
    MEDIUM = auto()    
    LARGE = auto()     
    FOUNDATION = auto()

class DataType(Enum):
    TEXT = auto()
    IMAGE = auto()
    GRAPH = auto()
    MULTIMODAL = auto()

class ModelRequirement(BaseModel):
    gpu_mem_gb: float = Field(..., ge=0)
    system_mem_gb: float = Field(..., ge=0.1)
    framework: str
    quantization: Optional[str]

class ModelMetadata(BaseModel):
    uid: str = Field(..., alias="_id", min_length=3)
    category: ModelCategory
    complexity: ModelComplexity
    data_types: List[DataType]
    version: str = Field("1.0.0", regex=r"^(auto|\d+\.\d+\.\d+)$")
    description: str = Field(..., min_length=10)
    authors: List[str]
    requirements: ModelRequirement

    @validator("version")
    def validate_version(cls, v):
        if v != "auto" and not semver.Version.is_valid(v):
            raise ValueError(f"Invalid semantic version: {v}")
        return v

# --- Model Implementations ---
class ModelImplementation:
    def __init__(self, metadata: ModelMetadata):
        self.metadata = metadata
        
    def train(self, dataset: Any):
        raise NotImplementedError
        
    def predict(self, input_data: Any) -> Any:
        raise NotImplementedError
        
    def score(self, X_test: Any, y_test: Any) -> float:
        raise NotImplementedError
        
    def save(self, path: str):
        """Atomic save with class preservation"""
        save_path = Path(path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # Store class name for loading
        meta = self.metadata.dict()
        meta["class_name"] = self.__class__.__name__
        
        # Atomic write
        temp_path = save_path.with_name(f"{save_path.name}.tmp")
        with open(temp_path / "metadata.json", "w") as f:
            json.dump(meta, f)
        
        self._save_weights(temp_path)
        temp_path.rename(save_path)
        
    def _save_weights(self, path: Path):
        """Framework-specific weight saving"""
        pass

    @classmethod
    def load(cls, path: str):
        """Universal loader with class resolution"""
        with open(Path(path) / "metadata.json") as f:
            meta = json.load(f)
        
        class_name = meta.pop("class_name")
        model_cls = globals()[class_name]
        
        metadata = ModelMetadata(**meta)
        instance = model_cls(metadata)
        instance._load_weights(Path(path))
        return instance

class DummyLinearModel(ModelImplementation):
    def __init__(self, metadata: ModelMetadata):
        super().__init__(metadata)
        self.a = 1.5
        self.b = 0.2
        
    def train(self, dataset: tuple[np.ndarray, np.ndarray]):
        X, y = dataset
        X_b = np.c_[np.ones((len(X), 1)), X]
        self.a, self.b = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
        
    def predict(self, input_data: Union[float, np.ndarray]):
        return self.a * input_data + self.b
        
    def score(self, X_test: np.ndarray, y_test: np.ndarray) -> float:
        preds = self.predict(X_test)
        return np.sqrt(np.mean((preds - y_test) ** 2))
        
    def _save_weights(self, path: Path):
        joblib.dump({"a": self.a, "b": self.b}, path / "weights.joblib")
        
    def _load_weights(self, path: Path):
        weights = joblib.load(path / "weights.joblib")
        self.a = weights["a"]
        self.b = weights["b"]

class PyTorchLinearModel(ModelImplementation):
    def __init__(self, metadata: ModelMetadata):
        super().__init__(metadata)
        self.model = torch.nn.Linear(1, 1)
        self.loss_fn = torch.nn.MSELoss()
        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01)
        
    def train(self, dataset: tuple[torch.Tensor, torch.Tensor]):
        X, y = dataset
        for _ in range(100):
            self.optimizer.zero_grad()
            pred = self.model(X)
            loss = self.loss_fn(pred, y)
            loss.backward()
            self.optimizer.step()
            
    def predict(self, input_data: float) -> float:
        with torch.no_grad():
            return self.model(torch.tensor([[input_data]])).item()
            
    def score(self, X_test: torch.Tensor, y_test: torch.Tensor) -> float:
        with torch.no_grad():
            preds = self.model(X_test)
            return torch.sqrt(torch.mean((preds - y_test) ** 2)).item()
            
    def _save_weights(self, path: Path):
        torch.save(self.model.state_dict(), path / "weights.pt")
        
    def _load_weights(self, path: Path):
        self.model.load_state_dict(torch.load(path / "weights.pt"))

class DummySymbolicModel(ModelImplementation):
    """Prime number detector with perfect validation accuracy"""
    def predict(self, input_data: int) -> bool:
        if input_data < 2:
            return False
        for i in range(2, int(np.sqrt(input_data)) + 1):
            if input_data % i == 0:
                return False
        return True

    def score(self, X_test: Any, y_test: Any) -> float:
        return 1.0  # Perfect accuracy by design

    def train(self, dataset: Any):
        pass  # No training needed

# --- Model Registry ---
class ModelRegistry:
    def __init__(self, registry_path: str = "./models"):
        self.registry_path = Path(registry_path)
        self._models: dict[str, dict[str, ModelImplementation]] = {}
        self._lock = threading.RLock()
        
    def register(self, model: ModelImplementation):
        with self._lock:
            uid = model.metadata.uid
            version = model.metadata.version
            
            # Handle auto-versioning
            if version == "auto":
                existing = self._models.get(uid, {})
                base_version = semver.Version.parse("1.0.0")
                if existing:
                    base_version = max(semver.Version.parse(v) for v in existing)
                    base_version = base_version.bump_minor()
                version = str(base_version)
                model.metadata.version = version
                
            # Validate version uniqueness
            if uid in self._models and version in self._models[uid]:
                raise ValueError(f"Model {uid} version {version} already exists")
                
            # Update registry
            self._models.setdefault(uid, {})[version] = model
            model.save(self.registry_path / uid / version)
            logger.info(f"Registered model {uid} v{version}")

    def get(self, model_id: str, version: str = "latest") -> Optional[ModelImplementation]:
        with self._lock:
            if model_id not in self._models:
                return None
                
            versions = self._models[model_id]
            
            # Resolve version
            if version == "latest":
                version = max(
                    (semver.Version.parse(v) for v in versions.keys()),
                    default=None
                )
                if not version:
                    return None
                version = str(version)
            elif version not in versions:
                return None
                
            # Check cache
            model = versions.get(version)
            if model:
                return model
                
            # Load from disk
            model_path = self.registry_path / model_id / version
            try:
                return ModelImplementation.load(model_path)
            except Exception as e:
                logger.error(f"Failed loading {model_id}/{version}: {str(e)}")
                return None

    def list_models(self) -> Dict[str, List[str]]:
        with self._lock:
            return {
                uid: list(versions.keys())
                for uid, versions in self._models.items()
            }

###########################
# knowledge.py - Memory   #
###########################

class MemoryType(Enum):
    SEMANTIC = auto()    
    EPISODIC = auto()    
    PROCEDURAL = auto()  
    CONTEXTUAL = auto()

class MemoryChunk(BaseModel):
    id: str = Field(..., min_length=3)
    content: Any
    type: MemoryType
    source: str
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    confidence: float = Field(0.0, ge=0.0, le=1.0)
    relations: Dict[str, List[str]] = Field(default_factory=dict)

class KnowledgeGraph:
    def __init__(self):
        self.graph = nx.DiGraph()
        
    def add_node(self, chunk: MemoryChunk):
        attrs = chunk.dict()
        self.graph.add_node(chunk.id, **attrs)
        
    def add_edge(self, source: str, target: str, relation: str):
        if self.graph.has_node(source) and self.graph.has_node(target):
            self.graph.add_edge(source, target, relation=relation)
        
    def query(self, pattern: Dict) -> List[MemoryChunk]:
        return [
            MemoryChunk(**self.graph.nodes[node])
            for node in self.graph.nodes
            if all(
                self.graph.nodes[node].get(k) == v
                for k, v in pattern.items()
            )
        ]

class KnowledgeManager:
    def __init__(self):
        self.graph = KnowledgeGraph()
        self.chunks: Dict[str, MemoryChunk] = {}
        self.lock = threading.RLock()
        
    def ingest(self, chunk: MemoryChunk):
        with self.lock:
            self.chunks[chunk.id] = chunk
            self.graph.add_node(chunk)
            logger.info(f"Ingested memory chunk {chunk.id}")

    def recall(self, pattern: Dict) -> List[MemoryChunk]:
        with self.lock:
            return self.graph.query(pattern)

##############################
# api.py - Interface Layer  #
##############################

class ModelServer:
    def __init__(self, registry: ModelRegistry):
        self.app = Flask(__name__)
        self.registry = registry
        
        @self.app.route("/models", methods=["GET"])
        def list_models():
            return jsonify(self.registry.list_models())
        
        @self.app.route("/register", methods=["POST"])
        def register_model():
            try:
                data = request.get_json()
                model = ModelFactory.create(
                    ModelCategory[data["category"]],
                    **data.get("config", {})
                )
                self.registry.register(model)
                return jsonify({
                    "status": "success",
                    "model_id": model.metadata.uid,
                    "version": model.metadata.version
                }), 201
            except KeyError as e:
                return jsonify({"error": f"Missing field: {str(e)}"}), 400
            except ValueError as e:
                return jsonify({"error": str(e)}), 400
            except Exception as e:
                logger.exception("Model registration failed")
                return jsonify({"error": "Internal server error"}), 500
        
        @self.app.route("/predict/<model_id>", methods=["POST"])
        def predict(model_id: str):
            version = request.args.get("version", "latest")
            try:
                model = self.registry.get(model_id, version)
                if not model:
                    return jsonify({"error": "Model not found"}), 404
                
                input_data = request.json
                result = model.predict(input_data)
                return jsonify({
                    "model": model_id,
                    "version": version,
                    "result": result
                })
            except Exception as e:
                logger.error(f"Prediction failed: {str(e)}")
                return jsonify({"error": "Prediction error"}), 500

##############################
# main.py - Orchestration    #
##############################

class GraceOrchestrator:
    def __init__(self):
        self.model_registry = ModelRegistry()
        self.knowledge_base = KnowledgeManager()
        self.server = ModelServer(self.model_registry)
        
    def initialize(self):
        """Cold-start initialization sequence"""
        logger.info("Initializing Grace System")
        
        # Load core models
        try:
            model = ModelFactory.create(
                ModelCategory.SYMBOLIC,
                uid="prime_detector",
                description="Core prime number detection system",
                requirements=ModelRequirement(
                    gpu_mem_gb=0,
                    system_mem_gb=0.1,
                    framework="none"
                )
            )
            self.model_registry.register(model)
        except Exception as e:
            logger.critical(f"Core model initialization failed: {str(e)}")
            raise
        
        # Prime knowledge base
        try:
            self.knowledge_base.ingest(MemoryChunk(
                id="grace_init",
                content="System initialized with core cognitive models",
                type=MemoryType.SEMANTIC,
                source="system",
                confidence=1.0
            ))
        except Exception as e:
            logger.error(f"Knowledge priming failed: {str(e)}")
        
        logger.info("Grace initialization complete")
    
    def run(self, host: str = "0.0.0.0", port: int = 5000):
        """Production run entrypoint"""
        self.initialize()
        self.server.app.run(host=host, port=port)
    
    def get_flask_app(self):
        """Expose WSGI app for production servers"""
        return self.server.app

# Production WSGI endpoint
app = GraceOrchestrator().get_flask_app()

if __name__ == "__main__":
    GraceOrchestrator().run()